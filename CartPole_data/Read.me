This folder contains all the data accumulated on the cartpole-v0 environment of gym
https://stackoverflow.com/questions/47036246/dqn-q-loss-not-converging
DQKSR stands for our Deep Q network in keras
ACKeras stands for the actor critic method created by phil in keras

Data naming for DQKSR
Environment-DQKSR-number_of_nodes-number_of_episodes

Data naming for ACKeras
Environment-ACKeras-nodes_layer_1_nodes_layer_2-number_of_episodes-lr-alfa_beta

a -punush denotes that the agent gets punished when there is traded without money or stocks
a -quiter denotes that the environments stops when traded without moeny or stocks (much like the cartpole environment stopts when the pole falls)

As for updates:
  x Some smoothener for the plots (data is very noisy)
  - Make a more automated approach to output files, such that errors in naming can be avoided
  - Make 1 file able to test both environments and both agents
  - Identify learning issues before moving on to new agents
  - When is the trading environment considered solved? (100 consecutive times a reward above 50?)
  - Add a flag to the environment creation if it will use a punishment or not
  - If the environment has a punishment add punish at the end of the save name
  x Perform data analysis/think of a clever way to do it as not all data sets have same length etc.
  - When running the same simulation twice be sure to add V2 behind it (maybe always start with V1 and make this also auto-updated)
  - Make learning rate and agent input for DQKSR
